---
layout: post
title: Deep Learning and the Demand for Interpretability
---

Deep learning has always been under fire for a lot of things in a lot of contexts. There is criticism about the arbitrariness of its hyperparameters and choice of architecture (Yann LeCun's [strong reaction](https://plus.google.com/+YannLeCunPhD/posts/gurGyczzsJ7) to a rejected paper from CVPR'12). There is also [criticism](http://lists.numenta.org/pipermail/nupic-theory_lists.numenta.org/2014-October/001453.html) about how they don't reflect the true functioning of what we know about the human brain. In the academic setting, another criticism I've noticed is how quite a few people suggest (I think, correctly) that "deep learning", if just dealt with as a method for stacking ad-hoc layers and loss functions, is not worth a student's time (see Ferenc Huszár's views [here](http://www.inference.vc/deep-learning-is-easy/)). Another popular area of discussion which has recently gained importance is about how deep learning is essentially a black box which may be fine for prediction tasks where only the results matter, but not in inference problems or tasks requiring an explanation of its results.

Although all these comments on deep learing belong to very diverse areas and are often over-generalized (deep learning in practice isn't a monolithic, standalone technique), in this post, I will write specifically about the notion of interpretability for these deep models. As the use of deep learning in real-life, decision-making systems increases, it becomes imperative that we are able to explain, to some degree, how our models come to the conclusions they do. But what exactly is intepretability and why is it needed at all? The remainder of this post discusses these two questions and finally explores some papers which try to make deep models more intepretable.

#### What is interpretability?

<br><br>
![Perils of using black boxes](/images/interpretability.jpg)
<br><br>

If a CNN model is to be made interpretable, what will make it so? Is it the features it generates, which should be interpretable, or the weights, or the choice of hyperparameters, or the learning algorithm, or the architecture itself? As far as supervised deep models are concerned, we know very well how the learning algorithm works to minimize the loss through gradient updates. We even have a fair idea of how the topology of these loss functions in the high dimensional space looks like and how we can possibly escape the local minima and saddle points and get to the optima. Does it mean that such a CNN model is interpretable? Or does knowing which specific neurons activate for an input and how the prediction accuracy varies when we obscure a part of the image make the model interpretable? Not necessarily. The questions above deal with various nuances of interpretability.

Zachary Lipton compiled his article on KDnuggets into a workshop paper called [The Mythos of Model Interpretability](https://arxiv.org/pdf/1606.03490.pdf) at the 2016 ICML Workshop on Human Interpretability of Machine Learning. In section 3, he defines two characteristics of an interpretable model: Transparency and Post-hoc Interpretability, each with more sub-parts. Transparency, he defines as *"opposite of blackbox-ness"* and *"some sense of understanding the mechanism by which the model works"*, which seem like a very broad definition and hughlights the difficulty in defining it. Post-hoc interetability, on the other hand, is simply the extraction and analysis of information from models after they have been learned. Clearly, the first one is the more interesting characteristic here, but also the one more difficult to achieve. He also argues in these sections that the posterboy of model interpretability in machine learning, ie, a decision tree, can be analyzed simply because of its size and its computational requirements and that there is nothing intrinsically interpretable about them. He says this is the case for most techniques and that there is often a tradeoff between constraining the size of the model or its computational requirements and its performance which in turn is often a good reason to ignore the opaqueness of the model.

I personally think that the task of defining interpretability formally is not the best way to go about the problem of making models more interpretable. Answering the question 'why interpretability', on the other hand, can give more specific and useful ways to approach the problem.

#### Why do we need interpretability?

A very popular thought in the machine learning circle goes like this:

*"The demand for complete interpretability from intelligent systems is overblown. Humans too are poor at explaining their decisions. We too are not completely interpretable."*

Although this statement glosses over a lot of specific legal, ethical and philosphical questions, it is important nevertheless to justify why or why not we need to invest time on transparent techniques which mostly will be transparent at the cost of performance. It helps to differentiate between the various motivations for such models.

##### 1. Interpretability for real-world applications

This sentence from a [blog](https://www.datanami.com/2017/03/15/scrutinizing-inscrutability-deep-learning/) is a good indicator of the need to have understandable models.
*"Try explaining an “ADAM Optimizer” to the judge when your GAN inadvertently crashes an autonomous vehicle into a crowd of innocent people."*

The reason why this is a good indicator isn't because of its correct technical understannding of the GANs or the machine learning models deployed in a self-driving cars, but because of exactly the opposite reason. Users of these models are usually people who don't understand these models. And they shouldn't need to. Users should be able to trust these systems for them to adpapted. It is interesting to note here that this motivation for interpretability is very different from the rest. The transparency that the model may provide might not serve any other purpose than to make the general public comfortable in using the system. This is in contrast to other motivations in the real-world where interpretability is largely a necessity. My first project in computer vision was to detect fire in industrial areas using surveillance cameras. The model consisted of a set of hand-engineered bag of features for the regions where motion was present followed by a binary classification using an SVM. I later discovered that it was not robust against adversarial video frames. A person walking past the camera with clothes of colours and textures similar to that of fire also triggered the system. But since the features were hand-engineered and small in number, I could identify why certain clothes predicted fire and subsequently managed to add more features like the flickering motion of fire pixels to handle the adversarial examples. On the other hand, deep CNNs have been known to be vulnerable to small, imperceptible adversarial changes in the input and don't allow for a robust analyses for why this is the case because, among other things, their distributed representations make it difficult to analyse how the neurons behave to adversarial examples. A lot of similar applications in healthcare and medicine also require justifications from the model as to why and how it produces its output. In fact this is one of the reasons why quite a few industries still use extremely simple linear models or decisin trees. However, it is important to keep track of the implications of using/not using more complex models by compromising transparency and explainability as it might be dependent on particular scenarios. Some would argue that even if the inner functioning of an autonomous car is partially opaque, knowing from empirical experiments, just the fact that its adoption will reduce the the number of accidents and deaths, is enough to give it a leeway in terms of the policy regulations. More generally, whether a use-case of machine learning needs to be interpretable, and if yes, then to what extent, must be decided on a case-by-case basis. This is something that was recently discussed at the panel discussion at the [Frontiers of Machine Learning](https://www.youtube.com/watch?v=09yQG_A1kHM).

##### 2. Interpretability for furthering research

Although many researchers don't agree with this, the theoretical foundations of many practices in deep learning is lacking. The immense potential and the fast growth of the field has led the researchers to come up with a lot of practical techniques to train, improve and modify these networks, with the theoretical understanding of them lagging behind. One of the motivations to invest time in the interpretability of these models is to identify the limitations and make theoretically sound improvements to the existing models. The next section talks very briefly about some of the works that I know of which try to do the same.


#### Work in deep learning and interpretability

This section will soon be updated